{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('./labeled_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisions = precision(y_true, y_pred)\n",
    "    recalls = recall(y_true, y_pred)\n",
    "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 [==============================] - 42s 63ms/step - loss: 0.2843 - accuracy: 0.8422 - f1: 0.8292 - precision: 0.8455 - recall: 0.8173 - val_loss: 0.2100 - val_accuracy: 0.8840 - val_f1: 0.8851 - val_precision: 0.8946 - val_recall: 0.8761\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 39s 63ms/step - loss: 0.1590 - accuracy: 0.9150 - f1: 0.9154 - precision: 0.9224 - recall: 0.9087 - val_loss: 0.2127 - val_accuracy: 0.8868 - val_f1: 0.8879 - val_precision: 0.8934 - val_recall: 0.8826\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 39s 63ms/step - loss: 0.1006 - accuracy: 0.9478 - f1: 0.9479 - precision: 0.9505 - recall: 0.9454 - val_loss: 0.2802 - val_accuracy: 0.8638 - val_f1: 0.8637 - val_precision: 0.8662 - val_recall: 0.8612\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 42s 68ms/step - loss: 0.0626 - accuracy: 0.9700 - f1: 0.9699 - precision: 0.9704 - recall: 0.9693 - val_loss: 0.2925 - val_accuracy: 0.8628 - val_f1: 0.8639 - val_precision: 0.8660 - val_recall: 0.8618\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 44s 70ms/step - loss: 0.0353 - accuracy: 0.9831 - f1: 0.9833 - precision: 0.9836 - recall: 0.9831 - val_loss: 0.3717 - val_accuracy: 0.8614 - val_f1: 0.8617 - val_precision: 0.8624 - val_recall: 0.8610\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 40s 64ms/step - loss: 0.0241 - accuracy: 0.9883 - f1: 0.9883 - precision: 0.9885 - recall: 0.9881 - val_loss: 0.3960 - val_accuracy: 0.8673 - val_f1: 0.8668 - val_precision: 0.8676 - val_recall: 0.8660\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 41s 65ms/step - loss: 0.0189 - accuracy: 0.9900 - f1: 0.9900 - precision: 0.9901 - recall: 0.9900 - val_loss: 0.4829 - val_accuracy: 0.8652 - val_f1: 0.8658 - val_precision: 0.8664 - val_recall: 0.8653\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 40s 65ms/step - loss: 0.0122 - accuracy: 0.9936 - f1: 0.9936 - precision: 0.9936 - recall: 0.9936 - val_loss: 0.5053 - val_accuracy: 0.8640 - val_f1: 0.8642 - val_precision: 0.8644 - val_recall: 0.8641\n",
      "Epoch 9/10\n",
      "620/620 [==============================] - 43s 70ms/step - loss: 0.0111 - accuracy: 0.9949 - f1: 0.9949 - precision: 0.9949 - recall: 0.9949 - val_loss: 0.6066 - val_accuracy: 0.8685 - val_f1: 0.8684 - val_precision: 0.8690 - val_recall: 0.8679\n",
      "Epoch 10/10\n",
      "620/620 [==============================] - 42s 68ms/step - loss: 0.0086 - accuracy: 0.9956 - f1: 0.9956 - precision: 0.9956 - recall: 0.9956 - val_loss: 0.7112 - val_accuracy: 0.8697 - val_f1: 0.8697 - val_precision: 0.8700 - val_recall: 0.8693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20635fe6fe0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the text and label columns\n",
    "text = data['tweet']\n",
    "labels = data['class']\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Padding\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# this for num of labels (harusnya ada 3)\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "one_hot_labels = to_categorical(encoded_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_sequence_length))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',f1,precision, recall])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "it consist of:\n",
    "- removing unique char (emoji, flags, non-alphabet char)\n",
    "- url links\n",
    "- stopwords\n",
    "- changing @xxx into user\n",
    "- null tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_emojis(raw_text):\n",
    "    text = html.unescape(raw_text)\n",
    "    text = demoji.replace(text, '')\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(raw_text):\n",
    "    tokenize = nltk.word_tokenize(raw_text)\n",
    "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "# Preprocessing\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def change_user(raw_text):\n",
    "    regex = r\"@([^ ]+)\"\n",
    "    text = re.sub(regex, \"user\", raw_text)\n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got ya bitch tip toeing on my hardwood floors \"\" &#128514; http://t.co/cOU2WQ5L4q\"\n",
      "pussy is a powerful drug @juanwmv  &#128517; #HappyHumpDay http://t.co/R8jsymiB5b\n",
      "...Son of a bitch took my Tic Tacs.\n",
      "\"@2015seniorprobs: I probably wouldn&#8217;t mind school as much if we didn&#8217;t have to deal with bitch ass teachers\"\". Retweet\n",
      "\"\"\"..All I wanna do is get money and fuck model bitches!\"\" - Russell Simmons\"\n",
      "@AutoWorId: Hennessey Venom GT &#128584; http://t.co/i8eGMnKaJ9 that's one sexy bitch\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    'got ya bitch tip toeing on my hardwood floors \"\" &#128514; http://t.co/cOU2WQ5L4q\"',\n",
    "    \"pussy is a powerful drug @j__nwmv \"\" &#128517; #HappyHumpDay http://t.co/R8jsymiB5b\",\n",
    "    \"...Son of a bitch took my Tic Tacs.\",\n",
    "    '\"@2015seniorprobs: I probably wouldn&#8217;t mind school as much if we didn&#8217;t have to deal with bitch ass teachers\"\". Retweet',\n",
    "    '\"\"\"..All I wanna do is get money and fuck model bitches!\"\" - Russell Simmons\"',\n",
    "    \"@AutoWorId: Hennessey Venom GT &#128584; http://t.co/i8eGMnKaJ9\"\" that's one sexy bitch\"\n",
    "]\n",
    "\n",
    "for x in test_list:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got ya bitch tip toeing on my hardwood floors \"\"  \"\n",
      "pussy is a powerful drug user   #HappyHumpDay \n",
      "...Son of a bitch took my Tic Tacs.\n",
      "\"user I probably wouldn’t mind school as much if we didn’t have to deal with bitch ass teachers\"\". Retweet\n",
      "\"\"\"..All I wanna do is get money and fuck model bitches!\"\" - Russell Simmons\"\n",
      "user Hennessey Venom GT   that's one sexy bitch\n"
     ]
    }
   ],
   "source": [
    "def remove_noise(datas):\n",
    "    clean = []\n",
    "    # change the @xxx into \"user\"\n",
    "    clean = [change_user(text) for text in datas]\n",
    "    # remove emojis (specifically unicode emojis)\n",
    "    clean = [remove_emojis(text) for text in clean]\n",
    "    # remove urls\n",
    "    clean = [remove_url(text) for text in clean]\n",
    "    # remove stopwords\n",
    "    # clean = [remove_stopwords(text) for text in clean]\n",
    "    return clean\n",
    "\n",
    "test_list = remove_noise(test_list)\n",
    "for x in test_list:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other\n",
      "in\n",
      "ll\n",
      "s\n",
      "o\n",
      "same\n",
      "shan't\n",
      "is\n",
      "a\n",
      "herself\n",
      "ours\n",
      "yourselves\n",
      "through\n",
      "couldn't\n",
      "wouldn\n",
      "you're\n",
      "which\n",
      "its\n",
      "was\n",
      "over\n",
      "theirs\n",
      "their\n",
      "those\n",
      "doing\n",
      "shouldn\n",
      "didn't\n",
      "she\n",
      "isn't\n",
      "so\n",
      "too\n",
      "into\n",
      "once\n",
      "being\n",
      "an\n",
      "isn\n",
      "they\n",
      "more\n",
      "you\n",
      "until\n",
      "your\n",
      "all\n",
      "hasn't\n",
      "hers\n",
      "been\n",
      "am\n",
      "his\n",
      "ma\n",
      "when\n",
      "mightn\n",
      "only\n",
      "mightn't\n",
      "did\n",
      "don't\n",
      "against\n",
      "wasn\n",
      "because\n",
      "re\n",
      "about\n",
      "the\n",
      "mustn't\n",
      "very\n",
      "haven\n",
      "you'll\n",
      "then\n",
      "will\n",
      "wasn't\n",
      "that'll\n",
      "have\n",
      "ourselves\n",
      "on\n",
      "needn\n",
      "doesn't\n",
      "mustn\n",
      "itself\n",
      "our\n",
      "themselves\n",
      "each\n",
      "didn\n",
      "yourself\n",
      "most\n",
      "him\n",
      "her\n",
      "how\n",
      "who\n",
      "nor\n",
      "that\n",
      "up\n",
      "whom\n",
      "m\n",
      "needn't\n",
      "it's\n",
      "my\n",
      "i\n",
      "ve\n",
      "does\n",
      "has\n",
      "wouldn't\n",
      "having\n",
      "after\n",
      "shouldn't\n",
      "at\n",
      "we\n",
      "them\n",
      "if\n",
      "or\n",
      "now\n",
      "hadn\n",
      "there\n",
      "why\n",
      "what\n",
      "me\n",
      "are\n",
      "should've\n",
      "yours\n",
      "some\n",
      "than\n",
      "haven't\n",
      "but\n",
      "for\n",
      "d\n",
      "down\n",
      "by\n",
      "just\n",
      "both\n",
      "with\n",
      "he\n",
      "you've\n",
      "from\n",
      "won\n",
      "further\n",
      "don\n",
      "couldn\n",
      "aren't\n",
      "below\n",
      "had\n",
      "between\n",
      "to\n",
      "be\n",
      "hadn't\n",
      "before\n",
      "while\n",
      "out\n",
      "y\n",
      "himself\n",
      "do\n",
      "off\n",
      "and\n",
      "as\n",
      "under\n",
      "t\n",
      "weren\n",
      "you'd\n",
      "myself\n",
      "she's\n",
      "again\n",
      "such\n",
      "of\n",
      "no\n",
      "own\n",
      "ain\n",
      "won't\n",
      "this\n",
      "these\n",
      "during\n",
      "above\n",
      "doesn\n",
      "weren't\n",
      "hasn\n",
      "shan\n",
      "were\n",
      "aren\n",
      "few\n",
      "any\n",
      "not\n",
      "should\n",
      "here\n",
      "it\n",
      "can\n",
      "where\n"
     ]
    }
   ],
   "source": [
    "for x in stop_words:\n",
    "    print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- got ya bitch tip toeing hardwood floors `` '' ``\n",
    "- pussy powerful drug user # HappyHumpDay\n",
    "- ... Son bitch took Tic Tacs .\n",
    "- `` user probably ’ mind school much ’ deal bitch ass teachers '' '' . Retweet\n",
    "- `` `` '' .. wan na get money fuck model bitches ! '' '' - Russell Simmons ''\n",
    "- user Hennessey Venom GT 's one sexy bitch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Forgot why i put this. only god knows what I'm thinking back then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "496/496 [==============================] - 34s 64ms/step - loss: -2.4334 - accuracy: 0.7722 - val_loss: -4.0524 - val_accuracy: 0.7844\n",
      "Epoch 2/10\n",
      "496/496 [==============================] - 29s 59ms/step - loss: -5.3477 - accuracy: 0.7722 - val_loss: -6.8819 - val_accuracy: 0.7844\n",
      "Epoch 3/10\n",
      "496/496 [==============================] - 28s 56ms/step - loss: -8.1396 - accuracy: 0.7722 - val_loss: -9.7407 - val_accuracy: 0.7844\n",
      "Epoch 4/10\n",
      "496/496 [==============================] - 27s 55ms/step - loss: -10.9515 - accuracy: 0.7722 - val_loss: -12.6367 - val_accuracy: 0.7844\n",
      "Epoch 5/10\n",
      "496/496 [==============================] - 26s 53ms/step - loss: -13.7553 - accuracy: 0.7722 - val_loss: -15.4700 - val_accuracy: 0.7844\n",
      "Epoch 6/10\n",
      "496/496 [==============================] - 26s 52ms/step - loss: -16.5498 - accuracy: 0.7722 - val_loss: -18.3350 - val_accuracy: 0.7844\n",
      "Epoch 7/10\n",
      "496/496 [==============================] - 26s 52ms/step - loss: -19.3322 - accuracy: 0.7722 - val_loss: -21.1619 - val_accuracy: 0.7844\n",
      "Epoch 8/10\n",
      "496/496 [==============================] - 27s 55ms/step - loss: -22.1131 - accuracy: 0.7722 - val_loss: -24.0077 - val_accuracy: 0.7844\n",
      "Epoch 9/10\n",
      "496/496 [==============================] - 29s 58ms/step - loss: -24.8973 - accuracy: 0.7722 - val_loss: -26.8396 - val_accuracy: 0.7844\n",
      "Epoch 10/10\n",
      "496/496 [==============================] - 29s 59ms/step - loss: -27.6947 - accuracy: 0.7722 - val_loss: -29.7158 - val_accuracy: 0.7844\n",
      "155/155 [==============================] - 1s 7ms/step - loss: -29.1178 - accuracy: 0.7730\n",
      "155/155 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(text, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_length = max(len(seq) for seq in train_sequences)\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_sequence_length))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_sequences, train_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_sequences, test_labels)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
