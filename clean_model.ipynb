{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # read the csv\n",
    "import re # regex to detect username, url, html entity \n",
    "import nltk # to use word tokenize (split the sentence into words)\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords # to remove the stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of tweets: (24783, 7)\n"
     ]
    }
   ],
   "source": [
    "# initiate the path and read it\n",
    "dataset_path = \"./labeled_data.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.head()\n",
    "\n",
    "# dataset shape to know how many tweets in the datasets\n",
    "print(f\"num of tweets: {df.shape}\")\n",
    "\n",
    "# extract the text and labels\n",
    "tweet = list(df['tweet'])\n",
    "labels = list(df['class'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "1. preprocessing\n",
    "2. splitting\n",
    "3. tokenize & padding\n",
    "4. Create model & train\n",
    "5. evaluate\n",
    "\n",
    "---\n",
    "\n",
    "Preprocessing (cleaning the datasets):\n",
    "- remove html entity\n",
    "- change user tags (@xxx -> user)\n",
    "- remove urls\n",
    "- remove unnecessary  symbol ('', !, \", ') -> cause a lot of noise in the dataset\n",
    "- remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## notes: all of the function taking 1 text at a time\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# add rt to remove retweet in dataset (noise)\n",
    "stop_words.add(\"rt\")\n",
    "\n",
    "# remove html entity:\n",
    "def remove_entity(raw_text):\n",
    "    entity_regex = r\"&[^\\s;]+;\"\n",
    "    text = re.sub(entity_regex, \"\", raw_text)\n",
    "    return text\n",
    "\n",
    "# change the user tags\n",
    "def change_user(raw_text):\n",
    "    regex = r\"@([^ ]+)\"\n",
    "    text = re.sub(regex, \"user\", raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_hashtag(raw_text):\n",
    "    regex = r\"#([^ ]+)\"\n",
    "    text = re.sub(regex, \"\", raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# remove urls\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove unnecessary symbols\n",
    "def remove_noise_symbols(raw_text):\n",
    "    text = raw_text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"!\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"..\", '')\n",
    "    text = text.replace(\"/\", '')\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    text = text.replace(\"|\", '')\n",
    "    return text\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(raw_text):\n",
    "    tokenize = nltk.word_tokenize(raw_text)\n",
    "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "## this function in to clean all the dataset by utilizing all the function above\n",
    "def preprocess(datas):\n",
    "    clean = []\n",
    "    # change the @xxx into \"user\"\n",
    "    clean = [change_user(text) for text in datas]\n",
    "    # remove emojis (specifically unicode emojis)\n",
    "    clean = [remove_entity(text) for text in clean]\n",
    "    # remove urls\n",
    "    clean = [remove_url(text) for text in clean]\n",
    "    # remove  hashtag\n",
    "    clean = [remove_hashtag(text) for text in clean]\n",
    "    # remove trailing stuff\n",
    "    clean = [remove_noise_symbols(text) for text in clean]\n",
    "    # remove stopwords\n",
    "    clean = [remove_stopwords(text) for text in clean]\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the cleaning function\n",
    "clean_tweet = preprocess(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24777\n",
      "24777\n"
     ]
    }
   ],
   "source": [
    "# remove the null tweet and its labels(probably neutral tweet)\n",
    "counter = 0\n",
    "for i, tweet in enumerate(clean_tweet):\n",
    "    if not tweet:\n",
    "        clean_tweet.pop(i)\n",
    "        labels.pop(i)\n",
    "\n",
    "# check the final len\n",
    "print(len(labels))\n",
    "print(len(clean_tweet))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving clean tweets and reading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment this section if you planning to run many times\n",
    "# write\n",
    "with open(\"clean_tweet.txt\", 'w') as file:\n",
    "    file.write('\\n'.join(clean_tweet))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into train, validation and test (70:20:10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_tweet, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2478\n"
     ]
    }
   ],
   "source": [
    "split = len(y_test)//3\n",
    "print(split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing -> basically we use tokenisation for many things, its commonly used for feature extraction in preprocessing. btw idk how it works as feature extraction tho :(\n",
    "# declare the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# build the vocabulary based on train dataset\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenize the train and test dataset\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# vocabulary size (num of unique words) -> will be used in embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding -> to uniform the datas\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "# to test an outlier case (if one of the test dataset has longer length)\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hot_labels (idk whty tapi ini penting)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "y_train = to_categorical(y_train, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num test tweet: 4957\n",
      "num train tweet: 19826\n"
     ]
    }
   ],
   "source": [
    "# another look on the number of tweet in test and training data\n",
    "print(f\"num test tweet: {y_test.shape[0]}\")\n",
    "print(f\"num train tweet: {y_train.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some early stopping to get the right epoch\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# evaluation metrics recall, precision, and f1\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisions = precision(y_true, y_pred)\n",
    "    recalls = recall(y_true, y_pred)\n",
    "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dis if u want\n",
    "output_dim = 200\n",
    "\n",
    "# LSTM model architechture (CNN + LSTM)\n",
    "model = Sequential([\n",
    "    # embedding layer is like idk\n",
    "    Embedding(vocab_size, output_dim, input_length=max_length),\n",
    "    # lstm for xxx\n",
    "    LSTM(1024, dropout=0.3, recurrent_dropout=0.3),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # dense to connect the previous output with current layer\n",
    "    Dense(2048, activation=\"relu\"),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # this is output layer, with 3 class (0, 1, 2)\n",
    "    Dense(3, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1,precision, recall])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 26, 200)           3734200   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1024)              5017600   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              2099200   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 6147      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,857,147\n",
      "Trainable params: 10,857,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# checking the model parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "206/620 [========>.....................] - ETA: 6:56 - loss: 0.4656 - accuracy: 0.8383 - f1: 0.8213 - precision: 0.8457 - recall: 0.8029"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\OneDrive - Bina Nusantara\\Documents\\GitHub\\NLP_projectussy\\clean_model.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     X_train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     y_train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     batch_size \u001b[39m=\u001b[39;49m \u001b[39m32\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_test, y_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    callbacks=[early_stopping],\n",
    "    batch_size = 32,\n",
    "    epochs=1000,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\OneDrive - Bina Nusantara\\Documents\\GitHub\\NLP_projectussy\\clean_model.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#Predict\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m y_prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#Create confusion matrix and normalizes it over predicted (columns)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/OneDrive%20-%20Bina%20Nusantara/Documents/GitHub/NLP_projectussy/clean_model.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m result \u001b[39m=\u001b[39m confusion_matrix(y_test, y_prediction , normalize\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Predict\n",
    "y_prediction = model.predict(x_test)\n",
    "\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_prediction , normalize='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 2\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Print the best epoch (by val loss)\n",
    "best_epoch = early_stopping.stopped_epoch\n",
    "print(\"Best Epoch:\", best_epoch-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test 1 batch size 32\n",
    "##### output_dim = 100 (best epoch = 1, based on val loss)\n",
    "- loss: 0.4318\n",
    "- accuracy: 0.8523\n",
    "- f1: 0.8356\n",
    "- precision: 0.8592\n",
    "- recall: 0.8167\n",
    "- val_loss: 0.3283\n",
    "- val_accuracy: 0.8864\n",
    "- val_f1: 0.8851\n",
    "- val_precision: 0.9080\n",
    "- val_recall: 0.8641\n",
    "\n",
    "##### output_dim = 200 (best epoch = 1, based on val loss)\n",
    "- loss: 0.4174\n",
    "- accuracy: 0.8545\n",
    "- f1: 0.8421\n",
    "- precision: 0.8706\n",
    "- recall: 0.8185\n",
    "- val_loss: 0.3044\n",
    "- val_accuracy: 0.8913\n",
    "- val_f1: 0.8906\n",
    "- val_precision: 0.9078\n",
    "- val_recall: 0.8745\n",
    "\n",
    "##### LSTM (128), dense (256)\n",
    "- loss: 0.3906\n",
    "- accuracy: 0.8656\n",
    "- f1: 0.8559\n",
    "- precision: 0.8769\n",
    "- recall: 0.8384\n",
    "- val_loss: 0.2915\n",
    "- val_accuracy: 0.8987\n",
    "- val_f1: 0.8985\n",
    "- val_precision: 0.9111\n",
    "- val_recall: 0.8866\n",
    "\n",
    "##### LSTM (256), dense (512)\n",
    "- loss: 0.3818\n",
    "- accuracy: 0.8678\n",
    "- f1: 0.8617\n",
    "- precision: 0.8783\n",
    "- recall: 0.8466\n",
    "- val_loss: 0.3016\n",
    "- val_accuracy: 0.8943\n",
    "- val_f1: 0.8944\n",
    "- val_precision: 0.9017\n",
    "- val_recall: 0.8874\n",
    "\n",
    "##### LSTM (512), dense (1024)\n",
    "- loss: 0.3763\n",
    "- accuracy: 0.8688\n",
    "- f1: 0.8632\n",
    "- precision: 0.8813\n",
    "- recall: 0.8476\n",
    "- val_loss: 0.2958\n",
    "- val_accuracy: 0.8965\n",
    "- val_f1: 0.8965\n",
    "- val_precision: 0.9084\n",
    "- val_recall: 0.8852\n",
    "\n",
    "##### LSTM (1024), dense (2048)\n",
    "- loss: 0.2217\n",
    "- accuracy: 0.9233\n",
    "- f1: 0.9223\n",
    "- precision: 0.9281\n",
    "- recall: 0.9168\n",
    "- val_loss: 0.3078\n",
    "- val_accuracy: 0.8935\n",
    "- val_f1: 0.8936\n",
    "- val_precision: 0.9040\n",
    "- val_recall: 0.8836"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
