{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # read the csv\n",
    "import re # regex to detect username, url, html entity \n",
    "import nltk # to use word tokenize (split the sentence into words)\n",
    "from nltk.corpus import stopwords # to remove the stopwords\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('./labeled_data.csv')\n",
    "df.head()\n",
    "\n",
    "# extract the text and labels\n",
    "tweet = list(df['tweet'])\n",
    "labels = list(df['class'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "1. preprocessing\n",
    "2. splitting\n",
    "3. tokenize & padding\n",
    "4. Create model & train\n",
    "5. evaluate\n",
    "\n",
    "---\n",
    "\n",
    "Preprocessing (cleaning the datasets):\n",
    "- remove html entity\n",
    "- change user tags (@xxx -> user)\n",
    "- remove urls\n",
    "- remove unnecessary  symbol ('', !, \", ') -> cause a lot of noise in the dataset\n",
    "- remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## notes: all of the function taking 1 text at a time\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# add rt to remove retweet in dataset (noise)\n",
    "stop_words.add(\"rt\")\n",
    "\n",
    "# remove html entity:\n",
    "def remove_entity(raw_text):\n",
    "    entity_regex = r\"&[^\\s;]+;\"\n",
    "    text = re.sub(entity_regex, \"\", raw_text)\n",
    "    return text\n",
    "\n",
    "# change the user tags\n",
    "def change_user(raw_text):\n",
    "    regex = r\"@([^ ]+)\"\n",
    "    text = re.sub(regex, \"user\", raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove urls\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove unnecessary symbols\n",
    "def remove_noise_symbols(raw_text):\n",
    "    text = raw_text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"!\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"..\", '')\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(raw_text):\n",
    "    tokenize = nltk.word_tokenize(raw_text)\n",
    "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "## this function in to clean all the dataset by utilizing all the function above\n",
    "def preprocess(datas):\n",
    "    clean = []\n",
    "    # change the @xxx into \"user\"\n",
    "    clean = [change_user(text) for text in datas]\n",
    "    # remove emojis (specifically unicode emojis)\n",
    "    clean = [remove_entity(text) for text in clean]\n",
    "    # remove urls\n",
    "    clean = [remove_url(text) for text in clean]\n",
    "    # remove trailing stuff\n",
    "    clean = [remove_noise_symbols(text) for text in clean]\n",
    "    # remove stopwords\n",
    "    clean = [remove_stopwords(text) for text in clean]\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the cleaning function\n",
    "clean_tweet = preprocess(tweet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_tweet, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing -> basically we use tokenisation for many things, its commonly used for feature extraction in preprocessing. btw idk how it works as feature extraction tho :(\n",
    "# declare the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# build the vocabulary based on train dataset\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenize the train and test dataset\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# vocabulary size (num of unique words) -> will be used in embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding -> to uniform the datas\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "# to test an outlier case (if one of the test dataset has longer length)\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hot_labels (idk whty tapi ini penting)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "y_train = to_categorical(y_train, num_classes=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisions = precision(y_true, y_pred)\n",
    "    recalls = recall(y_true, y_pred)\n",
    "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model architechture (CNN + LSTM)\n",
    "model = Sequential([\n",
    "    # embedding layer is like idk\n",
    "    Embedding(vocab_size, 100, input_length=max_length),\n",
    "    # lstm for xxx\n",
    "    LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # dense to connect the previous output with current layer\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # this is output layer, with 3 class (0, 1, 2)\n",
    "    Dense(3, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1,precision, recall])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 [==============================] - 16s 23ms/step - loss: 0.4314 - accuracy: 0.8523 - f1: 0.8372 - precision: 0.8628 - recall: 0.8163 - val_loss: 0.3194 - val_accuracy: 0.8937 - val_f1: 0.8925 - val_precision: 0.9087 - val_recall: 0.8774\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 13s 21ms/step - loss: 0.2336 - accuracy: 0.9218 - f1: 0.9216 - precision: 0.9316 - recall: 0.9122 - val_loss: 0.3078 - val_accuracy: 0.8957 - val_f1: 0.8941 - val_precision: 0.9035 - val_recall: 0.8852\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 13s 21ms/step - loss: 0.1537 - accuracy: 0.9451 - f1: 0.9451 - precision: 0.9489 - recall: 0.9415 - val_loss: 0.3941 - val_accuracy: 0.8806 - val_f1: 0.8802 - val_precision: 0.8855 - val_recall: 0.8751\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 13s 21ms/step - loss: 0.1067 - accuracy: 0.9631 - f1: 0.9630 - precision: 0.9643 - recall: 0.9618 - val_loss: 0.5024 - val_accuracy: 0.8685 - val_f1: 0.8688 - val_precision: 0.8702 - val_recall: 0.8675\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 13s 21ms/step - loss: 0.0820 - accuracy: 0.9717 - f1: 0.9715 - precision: 0.9727 - recall: 0.9704 - val_loss: 0.5327 - val_accuracy: 0.8731 - val_f1: 0.8736 - val_precision: 0.8754 - val_recall: 0.8719\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 13s 21ms/step - loss: 0.0685 - accuracy: 0.9759 - f1: 0.9759 - precision: 0.9765 - recall: 0.9754 - val_loss: 0.6105 - val_accuracy: 0.8790 - val_f1: 0.8786 - val_precision: 0.8802 - val_recall: 0.8771\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 13s 21ms/step - loss: 0.0560 - accuracy: 0.9799 - f1: 0.9798 - precision: 0.9802 - recall: 0.9793 - val_loss: 0.6552 - val_accuracy: 0.8717 - val_f1: 0.8719 - val_precision: 0.8733 - val_recall: 0.8705\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 13s 21ms/step - loss: 0.0510 - accuracy: 0.9814 - f1: 0.9814 - precision: 0.9820 - recall: 0.9809 - val_loss: 0.7270 - val_accuracy: 0.8687 - val_f1: 0.8685 - val_precision: 0.8690 - val_recall: 0.8681\n",
      "Epoch 9/10\n",
      "620/620 [==============================] - 13s 22ms/step - loss: 0.0465 - accuracy: 0.9835 - f1: 0.9835 - precision: 0.9839 - recall: 0.9830 - val_loss: 0.7920 - val_accuracy: 0.8618 - val_f1: 0.8621 - val_precision: 0.8630 - val_recall: 0.8612\n",
      "Epoch 10/10\n",
      "620/620 [==============================] - 13s 21ms/step - loss: 0.0404 - accuracy: 0.9855 - f1: 0.9855 - precision: 0.9859 - recall: 0.9852 - val_loss: 0.9127 - val_accuracy: 0.8701 - val_f1: 0.8700 - val_precision: 0.8707 - val_recall: 0.8693\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
