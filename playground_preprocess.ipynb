{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords # to remove the stopwords\n",
    "import demoji # demoji is used to remove pure emojis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter user detection (@xxx) test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this @hotstr bitch is bullying the hell out of @.ki.is23\n",
      "@332df you want to help @_7sad\n",
      "this @ is dumb things\n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "sample_text = [\n",
    "    \"this @hotstr bitch is bullying the hell out of @.ki.is23\",\n",
    "    \"@332df you want to help @_7sad\",\n",
    "    \"this @ is dumb things\",\n",
    "]\n",
    "\n",
    "# regex to detect the username (@xxx)\n",
    "regex = r\"@([^ ]+)\"\n",
    "\n",
    "# before preprocess\n",
    "for x in sample_text:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this user bitch is bullying the hell out of user\n",
      "user you want to help user\n",
      "this @ is dumb things\n"
     ]
    }
   ],
   "source": [
    "# execute the process\n",
    "for idx, string in enumerate(sample_text):\n",
    "    sample_text[idx] = re.sub(regex, \"user\", string)\n",
    "    \n",
    "# after preprocess\n",
    "for x in sample_text:\n",
    "    print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji remove test\n",
    "2 cases:\n",
    "- if the emoji is pure emoji\n",
    "- the emoji & ect is still on html entities like &#128584; or &#8217;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1:\n",
    "\n",
    "if the emoji is pure emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this @hotstr bitch is bullying some üè≥Ô∏è‚Äçüåà kids üíÄ\n",
      "üéâüéâüéâ happy birthday @_7sad\n",
      "why my life is sadü•π, maybe i should rethink my decisionü§î\n"
     ]
    }
   ],
   "source": [
    "# this is when the emoji is pure emoji\n",
    "sample_text = [\n",
    "    \"this @hotstr bitch is bullying some üè≥Ô∏è‚Äçüåà kids üíÄ\",\n",
    "    \"üéâüéâüéâ happy birthday @_7sad\",\n",
    "    \"why my life is sadü•π, maybe i should rethink my decisionü§î\",\n",
    "]\n",
    "\n",
    "# before preprocess\n",
    "for x in sample_text:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this @hotstr bitch is bullying some  kids \n",
      " happy birthday @_7sad\n",
      "why my life is sadü•π, maybe i should rethink my decision\n"
     ]
    }
   ],
   "source": [
    "# execute the emoji remover\n",
    "for idx, text in enumerate(sample_text):\n",
    "    sample_text[idx] = demoji.replace(text, \"\")\n",
    "\n",
    "# after emoji is removed\n",
    "for x in sample_text:\n",
    "    print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2:\n",
    "\n",
    "The emoji is in the form of html entities, we can use regex to remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got ya bitch tip toeing on my hardwood floors \"\"  http://t.co/cOU2WQ5L4q\"\n"
     ]
    }
   ],
   "source": [
    "entities_text = 'got ya bitch tip toeing on my hardwood floors \"\" &#128514; http://t.co/cOU2WQ5L4q\"'\n",
    "entity_regex = r\"&[^\\s;]+;\"\n",
    "text = re.sub(entity_regex, \"\", entities_text)\n",
    "print(text)\n",
    "\n",
    "# after this you can use the demoji library to remove the emojis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Url deletion test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is some sketchy links tokopedia.com/cart idk if safe or n lol\n",
      "http://t.co/cOU2WQ5L4q, this is some good shit\n",
      "hi babe join my class in bit.ly/kemanggisansus99\n"
     ]
    }
   ],
   "source": [
    "text = \"this a good resource https://www.google.com/search?client=firefox-b-d&q=k+means+clusterin, and i love it\"\n",
    "sample_text = [\n",
    "    \"this is some sketchy links tokopedia.com/cart idk if safe or n lol\",\n",
    "    \"http://t.co/cOU2WQ5L4q, this is some good shit\",\n",
    "    \"hi babe join my class in bit.ly/kemanggisansus99\"\n",
    "]\n",
    "url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))\"\n",
    "\n",
    "## single test\n",
    "# text = re.sub(url_regex, '', text)\n",
    "# print(text)\n",
    "\n",
    "# before preprocess\n",
    "for x in sample_text:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is some sketchy links  idk if safe or n lol\n",
      ", this is some good shit\n",
      "hi babe join my class in \n"
     ]
    }
   ],
   "source": [
    "for idx, text in enumerate(sample_text):\n",
    "    sample_text[idx] = re.sub(url_regex, '', text)\n",
    "\n",
    "# after preprocess\n",
    "for x in sample_text:\n",
    "    print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the trailing noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = [\n",
    "    \"...Son of a bitch took my Tic Tacs.\",\n",
    "    '\"\"\"..All I wanna do is get money and fuck model bitches!\"\" - Russell Simmons\"',\n",
    "]\n",
    "\n",
    "## if you want more \n",
    "# sample2 = [\n",
    "#     'got ya bitch tip toeing on my hardwood floors \"\"  http://t.co/cOU2WQ5L4q\"',\n",
    "#     'pussy is a powerful drug @juanwmv   #HappyHumpDay http://t.co/R8jsymiB5b',\n",
    "#     '...Son of a bitch took my Tic Tacs.',\n",
    "#     '\"@2015seniorprobs: I probably wouldn‚Äôt mind school as much if we didn‚Äôt have to deal with bitch ass teachers\"\". Retweet',\n",
    "#     '\"\"\"..All I wanna do is get money and fuck model bitches!\"\" - Russell Simmons\"',\n",
    "#     \"@AutoWorId: Hennessey Venom GT  http://t.co/i8eGMnKaJ9 that's one sexy bitch\",    \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".Son of a bitch took my Tic Tacs.\n",
      "All I wanna do is get money and fuck model bitches! - Russell Simmons\n"
     ]
    }
   ],
   "source": [
    "for x in sample_text:\n",
    "    ans = x.replace('\"', '')\n",
    "    ans = ans.replace(\"'\", '')\n",
    "    ans = ans.replace(\"..\", '')\n",
    "    print(ans)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the words and padding\n",
    "Tokenize:\n",
    "- changing text to number (idk why, but maybe to accelerate the calculation)\n",
    "\n",
    "Padding:\n",
    "- lstm input need to be consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text for tokenization\n",
    "sample_text = [\n",
    "    \"user woman shouldnt complain cleaning house . man always take trash .\",\n",
    "    \"user boy dats cold.tyga dwn bad cuffin dat hoe 1st place\",\n",
    "    \"user Dawg user ever fuck bitch start cry ? confused shit\",\n",
    "    \"user user look like tranny\",\n",
    "    \"user shit hear might true might faker bitch told ya\",\n",
    "    \"user shit blows meclaim faithful somebody still fucking hoes\",\n",
    "    \"user sit HATE another bitch got much shit going\",\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sample_text)\n",
    "tokenized_words = tokenizer.texts_to_sequences(sample_text)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token len =  10\n",
      "real       = user woman shouldnt complain cleaning house . man always take trash .\n",
      "tokenized  = [1, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "\n",
      "token len =  12\n",
      "real       = user boy dats cold.tyga dwn bad cuffin dat hoe 1st place\n",
      "tokenized  = [1, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "\n",
      "token len =  10\n",
      "real       = user Dawg user ever fuck bitch start cry ? confused shit\n",
      "tokenized  = [1, 25, 1, 26, 27, 3, 28, 29, 30, 2]\n",
      "\n",
      "token len =  5\n",
      "real       = user user look like tranny\n",
      "tokenized  = [1, 1, 31, 32, 33]\n",
      "\n",
      "token len =  10\n",
      "real       = user shit hear might true might faker bitch told ya\n",
      "tokenized  = [1, 2, 34, 4, 35, 4, 36, 3, 37, 38]\n",
      "\n",
      "token len =  9\n",
      "real       = user shit blows meclaim faithful somebody still fucking hoes\n",
      "tokenized  = [1, 2, 39, 40, 41, 42, 43, 44, 45]\n",
      "\n",
      "token len =  9\n",
      "real       = user sit HATE another bitch got much shit going\n",
      "tokenized  = [1, 46, 47, 48, 3, 49, 50, 2, 51]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## i forgot why i put this here, but i think its to prove something\n",
    "# print(len(tokenized_words))\n",
    "# print(len(sample_text))\n",
    "\n",
    "for i in range(len(tokenized_words)):\n",
    "    print(\"token len = \", len(tokenized_words[i]))\n",
    "    print(f\"real       = {sample_text[i]}\\ntokenized  = {tokenized_words[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding input uses the tokenized word\n",
    "# Padding\n",
    "max_length = max(len(seq) for seq in tokenized_words)\n",
    "padded_sequences = pad_sequences(tokenized_words, maxlen=max_length)\n",
    "max_sequence_length = max(len(seq) for seq in tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  1  5  6  7  8  9 10 11 12 13]\n",
      "[ 1 14 15 16 17 18 19 20 21 22 23 24]\n",
      "[ 0  0  1 25  1 26 27  3 28 29 30  2]\n",
      "[ 0  0  0  0  0  0  0  1  1 31 32 33]\n",
      "[ 0  0  1  2 34  4 35  4 36  3 37 38]\n",
      "[ 0  0  0  1  2 39 40 41 42 43 44 45]\n",
      "[ 0  0  0  1 46 47 48  3 49 50  2 51]\n"
     ]
    }
   ],
   "source": [
    "for x in padded_sequences:\n",
    "    print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing all\n",
    "1. change the user tags\n",
    "2. remove all html entity\n",
    "3. remove urls\n",
    "4. remove stopwords\n",
    "5. remove trailing\n",
    "6. padding and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"&#128514;&#128514;&#128514;&#128514;&#128514;&#128514;&#128514;&#128514;&#128514;&#128514;\n",
      "pussy is a powerful drug @juanwmv  &#128517; #HappyHumpDay http://t.co/R8jsymiB5b\n",
      "\"\"\"@Almightywayne__: @JetsAndASwisher @Gook____ bitch fuck u http://t.co/pXmGA68NC1\"\" maybe youll get better. Just http://t.co/TPreVwfq0S\"\"\n",
      "\"@2015seniorprobs: I probably wouldn&#8217;t mind school as much if we didn&#8217;t have to deal with bitch ass teachers\"\". Retweet\n",
      "\"\"\"..All I wanna do is get money and fuck model bitches!\"\" - Russell Simmons\"\n",
      "@BestProAdvice: The facts on tattoos...tattoo http://t.co/ZwnbhpDZ8e he's a pussy with not tattooing them nipples\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '\"\"\"&#128514;&#128514;&#128514;&#128514;&#128514;&#128514;&#128514;&#128514;&#128514;&#128514;',\n",
    "    \"pussy is a powerful drug @juanwmv \"\" &#128517; #HappyHumpDay http://t.co/R8jsymiB5b\",\n",
    "    '\"\"\"@Almightywayne__: @JetsAndASwisher @Gook____ bitch fuck u http://t.co/pXmGA68NC1\"\" maybe youll get better. Just http://t.co/TPreVwfq0S\"\"',\n",
    "    '\"@2015seniorprobs: I probably wouldn&#8217;t mind school as much if we didn&#8217;t have to deal with bitch ass teachers\"\". Retweet',\n",
    "    '\"\"\"..All I wanna do is get money and fuck model bitches!\"\" - Russell Simmons\"',\n",
    "    \"@BestProAdvice: The facts on tattoos...tattoo http://t.co/ZwnbhpDZ8e\"\" he's a pussy with not tattooing them nipples\",\n",
    "]\n",
    "\n",
    "# before\n",
    "for x in test_list:\n",
    "    print(x)\n",
    "# to contain the clean shit\n",
    "clean = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# add rt to remove retweet in dataset (noise)\n",
    "stop_words.add(\"rt\")\n",
    "\n",
    "def remove_emojis(raw_text):\n",
    "    entity_regex = r\"&[^\\s;]+;\"\n",
    "    text = re.sub(entity_regex, \"\", raw_text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(raw_text):\n",
    "    tokenize = nltk.word_tokenize(raw_text)\n",
    "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def change_user(raw_text):\n",
    "    regex = r\"@([^ ]+)\"\n",
    "    text = re.sub(regex, \"user\", raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_trailing_noise(raw_text):\n",
    "    text = raw_text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"!\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    # text = text.replace(\"‚Äù\", '')\n",
    "    text = text.replace(\"..\", '')\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_noise(datas):\n",
    "    clean = []\n",
    "    # change the @xxx into \"user\"\n",
    "    clean = [change_user(text) for text in datas]\n",
    "    # remove emojis (specifically unicode emojis)\n",
    "    clean = [remove_emojis(text) for text in clean]\n",
    "    # remove urls\n",
    "    clean = [remove_url(text) for text in clean]\n",
    "    # remove trailing stuff\n",
    "    clean = [remove_trailing_noise(text) for text in clean]\n",
    "    # remove stopwords\n",
    "    clean = [remove_stopwords(text) for text in clean]\n",
    "    return clean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...\n",
      "!!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!\n",
      "!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit\n",
      "!!!!!!!!! RT @C_G_Anderson: @viva_based she look like a tranny\n",
      "!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you hear about me might be true or it might be faker than the bitch who told it to ya &#57361;\n",
      "!!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just blows me..claim you so faithful and down for somebody but still fucking with hoes! &#128514;&#128514;&#128514;\"\n",
      "!!!!!!\"@__BrighterDays: I can not just sit up and HATE on another bitch .. I got too much shit going on!\"\n",
      "!!!!&#8220;@selfiequeenbri: cause I'm tired of you big bitches coming for us skinny girls!!&#8221;\n",
      "\" &amp; you might not get ya bitch back &amp; thats that \"\n",
      "\" @rhythmixx_ :hobbies include: fighting Mariam\"\n",
      "\n",
      "bitch\n",
      "\" Keeks is a bitch she curves everyone \" lol I walked into a conversation like this. Smh\n",
      "\" Murda Gang bitch its Gang Land \"\n",
      "\" So hoes that smoke are losers ? \" yea ... go on IG\n",
      "\" bad bitches is the only thing that i like \"\n",
      "\" bitch get up off me \"\n",
      "\" bitch nigga miss me with it \"\n",
      "\" bitch plz whatever \"\n",
      "\" bitch who do you love \"\n",
      "\" bitches get cut off everyday B \"\n",
      "\" black bottle &amp; a bad bitch \"\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./labeled_data.csv\")\n",
    "raw_dataset = df['tweet']\n",
    "raw_dataset = list(raw_dataset[:20])\n",
    "\n",
    "for x in raw_dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user woman shouldnt complain cleaning house . man always take trash .\n",
      "user boy dats cold.tyga dwn bad cuffin dat hoe 1st place\n",
      "user Dawg user ever fuck bitch start cry ? confused shit\n",
      "user user look like tranny\n",
      "user shit hear might true might faker bitch told ya\n",
      "user shit blows meclaim faithful somebody still fucking hoes\n",
      "user sit HATE another bitch got much shit going\n",
      "user cause Im tired big bitches coming us skinny girls\n",
      "might get ya bitch back thats\n",
      "user : hobbies include : fighting Mariam bitch\n",
      "Keeks bitch curves everyone lol walked conversation like . Smh\n",
      "Murda Gang bitch Gang Land\n",
      "hoes smoke losers ? yea . go IG\n",
      "bad bitches thing like\n",
      "bitch get\n",
      "bitch nigga miss\n",
      "bitch plz whatever\n",
      "bitch love\n",
      "bitches get cut everyday B\n",
      "black bottle bad bitch\n"
     ]
    }
   ],
   "source": [
    "good_tweet = remove_noise(raw_dataset)\n",
    "for x in good_tweet:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
